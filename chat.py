"""
äº¤äº’å¼å¯¹è¯è„šæœ¬ - ç”¨äºæµ‹è¯• MiniMind æ¨¡å‹æƒé‡

ä½¿ç”¨æ–¹æ³•ï¼š
    python chat.py --model_path out/pretrain_512_moe.pth
    python chat.py --model_path out/full_sft_512.pth --hidden_size 512
"""

import argparse
import os
import sys
import torch
from transformers import AutoTokenizer

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from model.model_minimind import MiniMindConfig, MiniMindForCausalLM


def load_model(model_path, hidden_size=512, use_moe=None, device='cuda'):
    """åŠ è½½æ¨¡å‹å’Œæƒé‡"""

    # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä¸º MoE æ¨¡å‹
    if use_moe is None:
        use_moe = '_moe' in model_path

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("\nå¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ï¼š")
        out_dir = os.path.dirname(model_path) or './out'
        if os.path.exists(out_dir):
            for f in os.listdir(out_dir):
                if f.endswith('.pth'):
                    print(f"  - {os.path.join(out_dir, f)}")
        sys.exit(1)

    # åˆ›å»ºæ¨¡å‹é…ç½®
    # intermediate_size ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„è®¡ç®—é€»è¾‘
    # intermediate_size = int(hidden_size * 8 / 3) ç„¶åå‘ä¸Šå–æ•´åˆ°64çš„å€æ•°
    intermediate_size_raw = int(hidden_size * 8 / 3)
    intermediate_size = 64 * ((intermediate_size_raw + 64 - 1) // 64)

    config = MiniMindConfig(
        hidden_size=hidden_size,
        num_hidden_layers=8,
        num_attention_heads=8,
        num_key_value_heads=2,
        intermediate_size=intermediate_size,
        vocab_size=6400,
        use_moe=use_moe,
    )

    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹é…ç½®ï¼š")
    print(f"  - Hidden size: {hidden_size}")
    print(f"  - Intermediate size: {intermediate_size}")
    print(f"  - Layers: {config.num_hidden_layers}")
    print(f"  - MoE: {'âœ… å¯ç”¨' if use_moe else 'âŒ ç¦ç”¨'}")
    print(f"  - Vocab size: {config.vocab_size}")

    # åˆ›å»ºæ¨¡å‹
    model = MiniMindForCausalLM(config)

    # åŠ è½½æƒé‡
    print(f"\nğŸ”„ åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
    state_dict = torch.load(model_path, map_location=device, weights_only=True)
    model.load_state_dict(state_dict)

    # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = model.to(device)
    model.eval()

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è®¾å¤‡: {device}")

    return model, config


def load_tokenizer(tokenizer_path='./model'):
    """åŠ è½½ tokenizer"""
    print(f"\nğŸ”¤ åŠ è½½ Tokenizer: {tokenizer_path}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            trust_remote_code=True,
            use_fast=True
        )
        print(f"âœ… Tokenizer åŠ è½½æˆåŠŸï¼è¯æ±‡é‡: {len(tokenizer)}")
        return tokenizer
    except Exception as e:
        print(f"âŒ åŠ è½½ Tokenizer å¤±è´¥: {e}")
        sys.exit(1)


def format_chat_prompt(messages, tokenizer):
    """å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸º ChatML æ ¼å¼"""
    # ä½¿ç”¨ tokenizer çš„ chat_template è¿›è¡Œæ ¼å¼åŒ–
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    return prompt


def generate_response(model, tokenizer, messages, max_new_tokens=256, temperature=0.7, top_p=0.9, top_k=50, device='cuda'):
    """ç”Ÿæˆå›å¤"""

    # æ ¼å¼åŒ–è¾“å…¥
    prompt = format_chat_prompt(messages, tokenizer)

    # Tokenize
    inputs = tokenizer(prompt, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            do_sample=temperature > 0,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True,
        )

    # è§£ç ï¼ˆåªå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    generated_ids = outputs[0][input_ids.shape[1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)

    return response.strip()


def interactive_chat(model, tokenizer, device='cuda', max_new_tokens=256, temperature=0.7, top_p=0.9, top_k=50):
    """äº¤äº’å¼å¯¹è¯å¾ªç¯"""

    print("\n" + "="*60)
    print("ğŸ¤– MiniMind äº¤äº’å¼å¯¹è¯")
    print("="*60)
    print("\nå‘½ä»¤ï¼š")
    print("  - è¾“å…¥æ¶ˆæ¯å¼€å§‹å¯¹è¯")
    print("  - è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("  - è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("  - è¾“å…¥ 'params' æŸ¥çœ‹/ä¿®æ”¹ç”Ÿæˆå‚æ•°")
    print("\nç”Ÿæˆå‚æ•°ï¼š")
    print(f"  - max_new_tokens: {max_new_tokens}")
    print(f"  - temperature: {temperature}")
    print(f"  - top_p: {top_p}")
    print(f"  - top_k: {top_k}")
    print("="*60 + "\n")

    # å¯¹è¯å†å²
    messages = []

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ğŸ‘¤ ä½ : ").strip()

            if not user_input:
                continue

            # å¤„ç†å‘½ä»¤
            if user_input.lower() in ['exit', 'quit']:
                print("\nğŸ‘‹ å†è§ï¼")
                break

            elif user_input.lower() == 'clear':
                messages = []
                print("ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º\n")
                continue

            elif user_input.lower() == 'params':
                print("\nå½“å‰ç”Ÿæˆå‚æ•°ï¼š")
                print(f"  max_new_tokens: {max_new_tokens}")
                print(f"  temperature: {temperature}")
                print(f"  top_p: {top_p}")
                print(f"  top_k: {top_k}")

                modify = input("\næ˜¯å¦ä¿®æ”¹å‚æ•°ï¼Ÿ(y/n): ").strip().lower()
                if modify == 'y':
                    try:
                        max_new_tokens = int(input(f"max_new_tokens [{max_new_tokens}]: ") or max_new_tokens)
                        temperature = float(input(f"temperature [{temperature}]: ") or temperature)
                        top_p = float(input(f"top_p [{top_p}]: ") or top_p)
                        top_k = int(input(f"top_k [{top_k}]: ") or top_k)
                        print("âœ… å‚æ•°å·²æ›´æ–°\n")
                    except ValueError:
                        print("âŒ å‚æ•°æ ¼å¼é”™è¯¯\n")
                continue

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": user_input})

            # ç”Ÿæˆå›å¤
            print("ğŸ¤– MiniMind: ", end="", flush=True)
            response = generate_response(
                model=model,
                tokenizer=tokenizer,
                messages=messages,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                device=device
            )
            print(response + "\n")

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
            messages.append({"role": "assistant", "content": response})

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}\n")
            # ç§»é™¤å¤±è´¥çš„æ¶ˆæ¯
            if messages and messages[-1]["role"] == "user":
                messages.pop()


def main():
    parser = argparse.ArgumentParser(description='MiniMind äº¤äº’å¼å¯¹è¯è„šæœ¬')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_path', type=str, default='out/pretrain_512_moe.pth',
                        help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--tokenizer_path', type=str, default='./model',
                        help='Tokenizer è·¯å¾„')
    parser.add_argument('--hidden_size', type=int, default=512,
                        help='éšè—å±‚å¤§å°')
    parser.add_argument('--use_moe', type=bool, default=None,
                        help='æ˜¯å¦ä½¿ç”¨ MoEï¼ˆé»˜è®¤ä»æ–‡ä»¶åè‡ªåŠ¨æ£€æµ‹ï¼‰')

    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--max_new_tokens', type=int, default=256,
                        help='æœ€å¤§ç”Ÿæˆ token æ•°')
    parser.add_argument('--temperature', type=float, default=0.7,
                        help='æ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰')
    parser.add_argument('--top_p', type=float, default=0.9,
                        help='nucleus sampling å‚æ•°')
    parser.add_argument('--top_k', type=int, default=50,
                        help='top-k sampling å‚æ•°')

    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                        help='è¿è¡Œè®¾å¤‡')

    args = parser.parse_args()

    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    print(f"\n{'='*60}")
    print("ğŸš€ MiniMind äº¤äº’å¼å¯¹è¯å¯åŠ¨")
    print(f"{'='*60}")
    print(f"è®¾å¤‡: {args.device}")
    if args.device == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"{'='*60}\n")

    # åŠ è½½ tokenizer
    tokenizer = load_tokenizer(args.tokenizer_path)

    # åŠ è½½æ¨¡å‹
    model, config = load_model(
        model_path=args.model_path,
        hidden_size=args.hidden_size,
        use_moe=args.use_moe,
        device=args.device
    )

    # å¼€å§‹äº¤äº’å¼å¯¹è¯
    interactive_chat(
        model=model,
        tokenizer=tokenizer,
        device=args.device,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k
    )


if __name__ == '__main__':
    main()
