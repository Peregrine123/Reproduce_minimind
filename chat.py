"""
äº¤äº’å¼å¯¹è¯è„šæœ¬ - ç”¨äºæµ‹è¯• MiniMind æ¨¡å‹æƒé‡

ä½¿ç”¨æ–¹æ³•ï¼š
    python chat.py --model_path out/pretrain_512_moe.pth
    python chat.py --model_path out/full_sft_512.pth --hidden_size 512
"""

import argparse
import os
import sys
import torch
from transformers import AutoTokenizer

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from model.model_minimind import MiniMindConfig, MiniMindForCausalLM


def load_model(model_path, hidden_size=512, use_moe=None, device='cuda'):
    """åŠ è½½æ¨¡å‹å’Œæƒé‡"""

    # è‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä¸º MoE æ¨¡å‹
    if use_moe is None:
        use_moe = '_moe' in model_path

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("\nå¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ï¼š")
        out_dir = os.path.dirname(model_path) or './out'
        if os.path.exists(out_dir):
            for f in os.listdir(out_dir):
                if f.endswith('.pth'):
                    print(f"  - {os.path.join(out_dir, f)}")
        sys.exit(1)

    # åˆ›å»ºæ¨¡å‹é…ç½®
    # intermediate_size ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„è®¡ç®—é€»è¾‘
    # intermediate_size = int(hidden_size * 8 / 3) ç„¶åå‘ä¸Šå–æ•´åˆ°64çš„å€æ•°
    intermediate_size_raw = int(hidden_size * 8 / 3)
    intermediate_size = 64 * ((intermediate_size_raw + 64 - 1) // 64)

    config = MiniMindConfig(
        hidden_size=hidden_size,
        num_hidden_layers=8,
        num_attention_heads=8,
        num_key_value_heads=2,
        intermediate_size=intermediate_size,
        vocab_size=6400,
        use_moe=use_moe,
    )

    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹é…ç½®ï¼š")
    print(f"  - Hidden size: {hidden_size}")
    print(f"  - Intermediate size: {intermediate_size}")
    print(f"  - Layers: {config.num_hidden_layers}")
    print(f"  - MoE: {'âœ… å¯ç”¨' if use_moe else 'âŒ ç¦ç”¨'}")
    print(f"  - Vocab size: {config.vocab_size}")

    # åˆ›å»ºæ¨¡å‹
    model = MiniMindForCausalLM(config)

    # åŠ è½½æƒé‡
    print(f"\nğŸ”„ åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
    state_dict = torch.load(model_path, map_location=device, weights_only=True)
    model.load_state_dict(state_dict)

    # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = model.to(device)
    model.eval()

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è®¾å¤‡: {device}")

    return model, config


def load_tokenizer(tokenizer_path='./model'):
    """åŠ è½½ tokenizer"""
    print(f"\nğŸ”¤ åŠ è½½ Tokenizer: {tokenizer_path}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            trust_remote_code=True,
            use_fast=True
        )
        print(f"âœ… Tokenizer åŠ è½½æˆåŠŸï¼è¯æ±‡é‡: {len(tokenizer)}")
        return tokenizer
    except Exception as e:
        print(f"âŒ åŠ è½½ Tokenizer å¤±è´¥: {e}")
        sys.exit(1)


def format_chat_prompt(messages, tokenizer):
    """å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸º ChatML æ ¼å¼"""
    # ä½¿ç”¨ tokenizer çš„ chat_template è¿›è¡Œæ ¼å¼åŒ–
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    return prompt


def generate_response(model, tokenizer, messages, max_new_tokens=256, temperature=0.7, top_p=0.9, top_k=50, device='cuda', debug=False):
    """ç”Ÿæˆå›å¤ - æ‰‹åŠ¨å®ç°ç”Ÿæˆé€»è¾‘ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜"""

    # æ ¼å¼åŒ–è¾“å…¥
    prompt = format_chat_prompt(messages, tokenizer)

    if debug:
        print(f"\n[DEBUG] Prompt: {prompt[:200]}...")
        print(f"[DEBUG] Vocab size: {len(tokenizer)}")
        print(f"[DEBUG] EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
        print(f"[DEBUG] BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")

    # Tokenize
    inputs = tokenizer(prompt, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)

    if debug:
        print(f"[DEBUG] Input IDs shape: {input_ids.shape}")
        print(f"[DEBUG] Input IDs range: [{input_ids.min().item()}, {input_ids.max().item()}]")

    # æ‰‹åŠ¨ç”Ÿæˆ
    generated_tokens = []
    past_key_values = None

    model.eval()
    with torch.no_grad():
        for step in range(max_new_tokens):
            # Forward pass
            outputs = model(
                input_ids=input_ids,
                past_key_values=past_key_values,
                use_cache=True
            )

            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
            next_token_logits = outputs.logits[:, -1, :]

            # æ¸©åº¦é‡‡æ ·
            if temperature > 0:
                next_token_logits = next_token_logits / temperature

                # Top-k è¿‡æ»¤
                if top_k > 0:
                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                    next_token_logits[indices_to_remove] = float('-inf')

                # Top-p (nucleus) è¿‡æ»¤
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)

                    # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ token
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0

                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    next_token_logits[indices_to_remove] = float('-inf')

                # é‡‡æ ·
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
            else:
                # Greedy decoding
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

            next_token_id = next_token.item()

            if debug and step < 10:  # åªæ˜¾ç¤ºå‰10ä¸ª token
                token_str = tokenizer.decode([next_token_id], skip_special_tokens=False)
                print(f"[DEBUG] Step {step}: Token ID {next_token_id} -> '{token_str}'")

            # æ£€æŸ¥ token ID æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if next_token_id >= len(tokenizer):
                print(f"\nâš ï¸  è­¦å‘Šï¼šç”Ÿæˆçš„ token ID {next_token_id} è¶…å‡ºè¯æ±‡è¡¨èŒƒå›´ [0, {len(tokenizer)-1}]")
                break

            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸ token
            if next_token_id == tokenizer.eos_token_id:
                if debug:
                    print(f"[DEBUG] EOS token generated at step {step}")
                break

            # æ·»åŠ åˆ°ç”Ÿæˆçš„ token åˆ—è¡¨
            generated_tokens.append(next_token_id)

            # å‡†å¤‡ä¸‹ä¸€è½®è¾“å…¥
            input_ids = next_token
            past_key_values = outputs.past_key_values

    # è§£ç ç”Ÿæˆçš„ token
    if generated_tokens:
        if debug:
            print(f"\n[DEBUG] Generated {len(generated_tokens)} tokens")
            print(f"[DEBUG] Token IDs: {generated_tokens[:20]}...")

        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        return response.strip()
    else:
        return ""


def interactive_chat(model, tokenizer, device='cuda', max_new_tokens=256, temperature=0.7, top_p=0.9, top_k=50, debug=False):
    """äº¤äº’å¼å¯¹è¯å¾ªç¯"""

    print("\n" + "="*60)
    print("ğŸ¤– MiniMind äº¤äº’å¼å¯¹è¯")
    print("="*60)
    print("\nå‘½ä»¤ï¼š")
    print("  - è¾“å…¥æ¶ˆæ¯å¼€å§‹å¯¹è¯")
    print("  - è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("  - è¾“å…¥ 'debug' åˆ‡æ¢è°ƒè¯•æ¨¡å¼")
    print("  - è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("  - è¾“å…¥ 'params' æŸ¥çœ‹/ä¿®æ”¹ç”Ÿæˆå‚æ•°")
    print("\nç”Ÿæˆå‚æ•°ï¼š")
    print(f"  - max_new_tokens: {max_new_tokens}")
    print(f"  - temperature: {temperature}")
    print(f"  - top_p: {top_p}")
    print(f"  - top_k: {top_k}")
    print(f"  - debug: {'âœ… å¯ç”¨' if debug else 'âŒ ç¦ç”¨'}")
    print("="*60 + "\n")

    # å¯¹è¯å†å²
    messages = []

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ğŸ‘¤ ä½ : ").strip()

            if not user_input:
                continue

            # å¤„ç†å‘½ä»¤
            if user_input.lower() in ['exit', 'quit']:
                print("\nğŸ‘‹ å†è§ï¼")
                break

            elif user_input.lower() == 'clear':
                messages = []
                print("ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º\n")
                continue

            elif user_input.lower() == 'debug':
                debug = not debug
                print(f"ğŸ› è°ƒè¯•æ¨¡å¼å·²{'å¯ç”¨' if debug else 'ç¦ç”¨'}\n")
                continue

            elif user_input.lower() == 'params':
                print("\nå½“å‰ç”Ÿæˆå‚æ•°ï¼š")
                print(f"  max_new_tokens: {max_new_tokens}")
                print(f"  temperature: {temperature}")
                print(f"  top_p: {top_p}")
                print(f"  top_k: {top_k}")
                print(f"  debug: {'âœ… å¯ç”¨' if debug else 'âŒ ç¦ç”¨'}")

                modify = input("\næ˜¯å¦ä¿®æ”¹å‚æ•°ï¼Ÿ(y/n): ").strip().lower()
                if modify == 'y':
                    try:
                        max_new_tokens = int(input(f"max_new_tokens [{max_new_tokens}]: ") or max_new_tokens)
                        temperature = float(input(f"temperature [{temperature}]: ") or temperature)
                        top_p = float(input(f"top_p [{top_p}]: ") or top_p)
                        top_k = int(input(f"top_k [{top_k}]: ") or top_k)
                        print("âœ… å‚æ•°å·²æ›´æ–°\n")
                    except ValueError:
                        print("âŒ å‚æ•°æ ¼å¼é”™è¯¯\n")
                continue

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": user_input})

            # ç”Ÿæˆå›å¤
            print("ğŸ¤– MiniMind: ", end="", flush=True)
            response = generate_response(
                model=model,
                tokenizer=tokenizer,
                messages=messages,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                device=device,
                debug=debug
            )
            print(response + "\n")

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
            messages.append({"role": "assistant", "content": response})

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}\n")
            import traceback
            if debug:
                traceback.print_exc()
            # ç§»é™¤å¤±è´¥çš„æ¶ˆæ¯
            if messages and messages[-1]["role"] == "user":
                messages.pop()


def main():
    parser = argparse.ArgumentParser(description='MiniMind äº¤äº’å¼å¯¹è¯è„šæœ¬')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_path', type=str, default='out/pretrain_512_moe.pth',
                        help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--tokenizer_path', type=str, default='./model',
                        help='Tokenizer è·¯å¾„')
    parser.add_argument('--hidden_size', type=int, default=512,
                        help='éšè—å±‚å¤§å°')
    parser.add_argument('--use_moe', type=bool, default=None,
                        help='æ˜¯å¦ä½¿ç”¨ MoEï¼ˆé»˜è®¤ä»æ–‡ä»¶åè‡ªåŠ¨æ£€æµ‹ï¼‰')

    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--max_new_tokens', type=int, default=256,
                        help='æœ€å¤§ç”Ÿæˆ token æ•°')
    parser.add_argument('--temperature', type=float, default=0.7,
                        help='æ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰')
    parser.add_argument('--top_p', type=float, default=0.9,
                        help='nucleus sampling å‚æ•°')
    parser.add_argument('--top_k', type=int, default=50,
                        help='top-k sampling å‚æ•°')

    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                        help='è¿è¡Œè®¾å¤‡')
    parser.add_argument('--debug', action='store_true',
                        help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„ç”Ÿæˆä¿¡æ¯')

    args = parser.parse_args()

    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    print(f"\n{'='*60}")
    print("ğŸš€ MiniMind äº¤äº’å¼å¯¹è¯å¯åŠ¨")
    print(f"{'='*60}")
    print(f"è®¾å¤‡: {args.device}")
    if args.device == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"{'='*60}\n")

    # åŠ è½½ tokenizer
    tokenizer = load_tokenizer(args.tokenizer_path)

    # åŠ è½½æ¨¡å‹
    model, config = load_model(
        model_path=args.model_path,
        hidden_size=args.hidden_size,
        use_moe=args.use_moe,
        device=args.device
    )

    # å¼€å§‹äº¤äº’å¼å¯¹è¯
    interactive_chat(
        model=model,
        tokenizer=tokenizer,
        device=args.device,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        debug=args.debug
    )


if __name__ == '__main__':
    main()
