# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

è¿™æ˜¯ä¸€ä¸ª MiniMind LLM çš„å¤ç°é¡¹ç›®ï¼ŒåŒ…å«äº†ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„å°å‹è¯­è¨€æ¨¡å‹å®ç°ã€‚é¡¹ç›®æ”¯æŒæ ‡å‡†çš„ Transformer ç»“æ„ä»¥åŠæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å¼ã€‚

**æ ¸å¿ƒç»„ä»¶**ï¼š
- æ¨¡å‹å®šä¹‰ï¼š`model/model_minimind.py` (466è¡Œ)
- æ•°æ®å¤„ç†ï¼š`dataset/lm_dataset.py` (233è¡Œ)
- è®­ç»ƒæµç¨‹ï¼š`triainer/train_pretrian.py` (211è¡Œ) + `triainer/train_full_sft.py` (230è¡Œ)
- ç»Ÿä¸€å…¥å£ï¼š`main.py` (144è¡Œ) - æ”¯æŒ Kaggle/Jupyter ç¯å¢ƒ

## Environment Setup

é¡¹ç›®ä½¿ç”¨ uv ä½œä¸ºåŒ…ç®¡ç†å™¨ï¼Œé…ç½®åœ¨ pyproject.toml ä¸­ï¼š

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
uv sync

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Windows)
.venv\Scripts\activate

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Linux/Mac)
source .venv/bin/activate
```

## Training Workflow

### ä¼˜åŒ–åçš„æ¨èé…ç½®ï¼ˆé’ˆå¯¹ T4 x 2ï¼‰

```bash
# é¢„è®­ç»ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰
python main.py --mode pretrain \
    --epochs 5 \
    --batch_size 32 \
    --learning_rate 2e-4 \
    --warmup_iters 2000 \
    --accumulation_steps 1

# æœ‰æ•ˆ batch_size = 32 Ã— 1 Ã— 2 = 64
# é¢„è®¡è®­ç»ƒæ—¶é—´ï¼šçº¦ 25 å°æ—¶ (åŸºäºå®æµ‹ 2 epoch = 10 å°æ—¶)
```

**å…³é”®æ”¹è¿›**ï¼š
- âœ… Epochs ä» 2 å¢åŠ åˆ° 5ï¼ˆè®­ç»ƒæ›´å……åˆ†ï¼Œé¿å…æ¬ æ‹Ÿåˆï¼‰
- âœ… Batch size ä» 16 å¢åŠ åˆ° 32ï¼ˆå……åˆ†åˆ©ç”¨ T4 æ˜¾å­˜ï¼‰
- âœ… Learning rate ä» 5e-4 é™è‡³ 2e-4ï¼ˆæé«˜ç¨³å®šæ€§ï¼‰
- âœ… Warmup ä» 0 å¢åŠ åˆ° 2000 æ­¥ï¼ˆé¿å…åˆæœŸéœ‡è¡ï¼‰
- âœ… Accumulation steps è®¾ä¸º 1ï¼ˆå‚æ•°æ›´æ–°æ›´é¢‘ç¹ï¼Œè®­ç»ƒæ›´å¿«ï¼Œæœ‰æ•ˆbatch=64é€‚åˆå°æ¨¡å‹ï¼‰

ğŸ“„ **è¯¦ç»†åˆ†æ**ï¼šå‚è§ `TRAINING_OPTIMIZATION.md`

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ç»Ÿä¸€å…¥å£ main.pyï¼ˆæ¨èç”¨äº Kaggle/Jupyterï¼‰

```bash
# é¢„è®­ç»ƒï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
python main.py --mode pretrain \
    --epochs 2 \
    --batch_size 32 \
    --learning_rate 5e-4 \
    --hidden_size 512 \
    --pretrain_data_path ./dataset/pretrain_hq.jsonl

# ç›‘ç£å¾®è°ƒ (SFT)
python main.py --mode sft \
    --epochs 2 \
    --batch_size 16 \
    --learning_rate 5e-7 \
    --hidden_size 512 \
    --sft_data_path ./dataset/sft_data.jsonl

# MoE è®­ç»ƒ
python main.py --mode pretrain \
    --use_moe True \
    --hidden_size 512

# åˆ†å¸ƒå¼è®­ç»ƒï¼ˆéœ€è¦é…åˆ torchrunï¼‰
torchrun --nproc_per_node=2 main.py \
    --mode pretrain \
    --ddp \
    --batch_size 32
```

**main.py ç‰¹ç‚¹**ï¼š
- è‡ªåŠ¨å¤„ç† Jupyter/Colab ç¯å¢ƒçš„ç‰¹æ®Šå‚æ•°ï¼ˆ`-f` å’Œ `.json` æ–‡ä»¶ï¼‰
- ä½¿ç”¨ `exec()` ç›´æ¥æ‰§è¡Œè®­ç»ƒè„šæœ¬ï¼Œè§£å†³ `__file__` æœªå®šä¹‰é—®é¢˜
- ç»Ÿä¸€å‚æ•°æ¥å£ï¼Œç®€åŒ–å‘½ä»¤è¡Œæ“ä½œ

### æ–¹æ³•äºŒï¼šç›´æ¥è°ƒç”¨è®­ç»ƒè„šæœ¬

```bash
# é¢„è®­ç»ƒ
python triainer/train_pretrian.py \
    --data_path ./dataset/pretrain_hq.jsonl \
    --epochs 2 \
    --batch_size 32 \
    --learning_rate 5e-4

# SFT è®­ç»ƒ
python triainer/train_full_sft.py \
    --data_path ./dataset/sft_data.jsonl \
    --epochs 2 \
    --batch_size 16 \
    --learning_rate 5e-7

# åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --nproc_per_node=2 triainer/train_pretrian.py \
    --ddp \
    --batch_size 32
```

### é€šç”¨è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--out_dir` | `./out` | æ¨¡å‹ä¿å­˜ç›®å½• |
| `--epochs` | `2` | è®­ç»ƒè½®æ•° |
| `--batch_size` | `16` (SFT) / `32` (Pretrain) | æ‰¹æ¬¡å¤§å° |
| `--learning_rate` | `5e-7` (SFT) / `5e-4` (Pretrain) | å­¦ä¹ ç‡ |
| `--hidden_size` | `512` | éšè—å±‚ç»´åº¦ |
| `--num_hidden_layers` | `8` | Transformer å±‚æ•° |
| `--max_seq_len` | `512` | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--use_moe` | `False` | æ˜¯å¦ä½¿ç”¨ MoE |
| `--accumulation_steps` | `1` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (æ¨è1ï¼Œæœ‰æ•ˆbatch=32Ã—1Ã—2=64) |
| `--grad_clip` | `1.0` | æ¢¯åº¦è£å‰ªé˜ˆå€¼ |
| `--warmup_iters` | `0` | å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•° |
| `--save_interval` | `500` | æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆæ­¥ï¼‰ |
| `--log_interval` | `100` | æ—¥å¿—è¾“å‡ºé—´éš”ï¼ˆæ­¥ï¼‰ |
| `--use_wandb` | - | å¯ç”¨ WandB æ—¥å¿— |
| `--ddp` | - | å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ |

## Code Quality Tools

é¡¹ç›®é…ç½®äº† Ruff + basedpyright ç»„åˆï¼š

```bash
# æ£€æŸ¥ä»£ç é£æ ¼å’Œè´¨é‡
ruff check

# è‡ªåŠ¨ä¿®å¤å¯ä¿®å¤çš„é—®é¢˜
ruff check --fix

# æ ¼å¼åŒ–ä»£ç 
ruff format

# å®Œæ•´çš„ä»£ç è´¨é‡æ£€æŸ¥æµç¨‹
ruff check --fix && ruff format

# æ£€æŸ¥ç‰¹å®šæ–‡ä»¶
ruff check model/model_minimind.py
```

**Ruff é…ç½®è¦ç‚¹**ï¼š
- è¡Œé•¿åº¦é™åˆ¶ï¼š88 å­—ç¬¦
- å¯ç”¨è§„åˆ™ï¼špycodestyle, pyflakes, isort, pep8-naming, pyupgrade, mccabe, pylint, flake8-bugbear
- Import æ’åºï¼šè‡ªåŠ¨å¯¹ `model` å’Œ `triainer` åŒ…è¿›è¡Œä¼˜å…ˆæ’åº
- å¿½ç•¥è§„åˆ™ï¼šE501ï¼ˆè¡Œé•¿ï¼‰ï¼ŒPLR0913/0912/0915ï¼ˆå¤æ‚åº¦ï¼‰ï¼ŒN803/N806ï¼ˆå‘½åï¼‰

## Core Architecture

### æ¨¡å‹ç»“æ„ (`model/model_minimind.py`)

**æ ¸å¿ƒç±»**ï¼š
- `MiniMindConfig`: æ¨¡å‹é…ç½®ç±»ï¼Œæ”¯æŒæ ‡å‡†å’Œ MoE é…ç½®
- `MiniMindForCausalLM`: ä¸»æ¨¡å‹ç±»ï¼Œç”¨äºå› æœè¯­è¨€å»ºæ¨¡
- `MiniMindModel`: Transformer ä¸»ä½“ï¼Œå¤šå±‚å †å 
- `MiniMindBlock`: å•ä¸ª Transformer å—ï¼ˆAttention + FFNï¼‰
- `Attention`: æ”¯æŒ Flash Attention å’Œ RoPE ä½ç½®ç¼–ç 
- `FeedForward`: SwiGLU å‰é¦ˆç½‘ç»œ
- `MoEGate` + `MoEFeedForward`: æ··åˆä¸“å®¶æ¨¡å—
- `RMSNorm`: RMS å½’ä¸€åŒ–å±‚

**å…³é”®æŠ€æœ¯**ï¼š
- **RoPE ä½ç½®ç¼–ç **ï¼šæ—‹è½¬ä½ç½®ç¼–ç ï¼ˆ`rope_theta=1e6`ï¼‰
- **Flash Attention**ï¼šé«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—ï¼Œæ”¯æŒåˆ†é¡µ KV ç¼“å­˜
- **GQA**ï¼šåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆ`num_key_value_heads=2`ï¼Œ`num_attention_heads=8`ï¼‰
- **MoE**ï¼š
  - `num_experts_per_tok=2`ï¼šæ¯ä¸ª token é€‰æ‹© 2 ä¸ªä¸“å®¶
  - `n_routed_experts=4`ï¼šæ€»å…± 4 ä¸ªè·¯ç”±ä¸“å®¶
  - `n_shared_experts=1`ï¼š1 ä¸ªå…±äº«ä¸“å®¶
  - è¾…åŠ©æŸå¤±ï¼ˆaux_lossï¼‰ï¼šå¹³è¡¡ä¸“å®¶è´Ÿè½½

**æ¨¡å‹é…ç½®é»˜è®¤å€¼**ï¼š
```python
hidden_size=512
num_hidden_layers=8
num_attention_heads=8
num_key_value_heads=2
max_position_embeddings=32768
vocab_size=6400
intermediate_size=1536  # 3 * hidden_size
```

### æ•°æ®å¤„ç† (`dataset/lm_dataset.py`)

**æ”¯æŒçš„æ•°æ®é›†ç±»å‹**ï¼š
1. `PretrianDataset`ï¼šé¢„è®­ç»ƒæ•°æ®é›†ï¼ˆJSONL æ ¼å¼ï¼‰
2. `SFTDataset`ï¼šç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼ˆChatML æ ¼å¼ï¼‰
3. `DPODataset`ï¼šç›´æ¥åå¥½ä¼˜åŒ–æ•°æ®é›†
4. `RLAIFDataset`ï¼šå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ•°æ®é›†

**æ•°æ®æ ¼å¼**ï¼š
- è¾“å…¥ï¼šJSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
- è¾“å‡ºï¼š`(X, Y, loss_mask)` ä¸‰å…ƒç»„
  - `X`ï¼šè¾“å…¥ token ID
  - `Y`ï¼šç›®æ ‡ token ID
  - `loss_mask`ï¼šæŸå¤±æ©ç ï¼ˆåŒºåˆ†çœŸå® token å’Œ paddingï¼‰
- èŠå¤©æ¨¡æ¿ï¼šChatML æ ¼å¼ï¼ˆ`<|im_start|>user/assistant/system<|im_end|>`ï¼‰

**Tokenizer**ï¼š
- è¯æ±‡é‡ï¼š6400
- ç‰¹æ®Š tokenï¼š`<|endoftext|>`(0), `<|im_start|>`(1), `<|im_end|>`(2)
- æ–‡ä»¶ï¼š`model/tokenizer.json`, `model/vocab.json`, `model/tokenizer_config.json`

### è®­ç»ƒæµç¨‹æ¶æ„

**å­¦ä¹ ç‡è°ƒåº¦**ï¼š
- ç­–ç•¥ï¼šCosine Annealing with Warmup
- å®ç°ï¼š`get_lr()` å‡½æ•°ï¼ˆåœ¨ `train_pretrian.py:25` å’Œ `train_full_sft.py:24`ï¼‰
- å…¬å¼ï¼š
  ```python
  # Warmup é˜¶æ®µ
  lr = max_lr * it / warmup_iters

  # Decay é˜¶æ®µ
  decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)
  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
  lr = min_lr + coeff * (max_lr - min_lr)
  ```

**è®­ç»ƒå¾ªç¯**ï¼š
1. åŠ è½½æ•°æ® â†’ 2. å‰å‘ä¼ æ’­ â†’ 3. è®¡ç®—æŸå¤± â†’ 4. åå‘ä¼ æ’­ â†’ 5. æ¢¯åº¦ç´¯ç§¯ â†’ 6. æ¢¯åº¦è£å‰ª â†’ 7. ä¼˜åŒ–å™¨æ›´æ–° â†’ 8. ä¿å­˜æ£€æŸ¥ç‚¹

**æ··åˆç²¾åº¦è®­ç»ƒ**ï¼š
- ä½¿ç”¨ `torch.cuda.amp.autocast()` è‡ªåŠ¨æ··åˆç²¾åº¦
- ä½¿ç”¨ `GradScaler` é˜²æ­¢æ¢¯åº¦ä¸‹æº¢
- æ”¯æŒ `bfloat16` å’Œ `float16`

**åˆ†å¸ƒå¼è®­ç»ƒ**ï¼š
- ä½¿ç”¨ `torch.nn.parallel.DistributedDataParallel` (DDP)
- è‡ªåŠ¨åŒæ­¥æ¢¯åº¦å’Œæ¨¡å‹å‚æ•°
- æ”¯æŒ `torchrun` å¯åŠ¨

## Model Checkpoints

**ä¿å­˜æ ¼å¼**ï¼š
- é¢„è®­ç»ƒæ¨¡å‹ï¼š`{out_dir}/pretrain_{hidden_size}[_moe].pth`
- SFT æ¨¡å‹ï¼š`{out_dir}/full_sft_{hidden_size}[_moe].pth`
- å­˜å‚¨ç²¾åº¦ï¼šåŠç²¾åº¦ï¼ˆ`.half()`ï¼‰

**æ£€æŸ¥ç‚¹å†…å®¹**ï¼š
- æ¨¡å‹æƒé‡ï¼š`model.state_dict()`
- ä¼˜åŒ–å™¨çŠ¶æ€ï¼šé€šå¸¸ä¸ä¿å­˜ï¼ˆä»…æ¨ç†ï¼‰

**åŠ è½½æ–¹å¼**ï¼š
```python
# SFT è®­ç»ƒä¼šè‡ªåŠ¨åŠ è½½å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹
pretrain_model_path = f"{out_dir}/pretrain_{hidden_size}{'_moe' if use_moe else ''}.pth"
model.load_state_dict(torch.load(pretrain_model_path))
```

## Data Requirements

**é¢„è®­ç»ƒæ•°æ®**ï¼š
- æ–‡ä»¶ï¼š`dataset/pretrain_hq.jsonl` (1.6GB)
- æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å« `text` å­—æ®µ
- ç¤ºä¾‹ï¼š`{"text": "è¿™æ˜¯ä¸€æ®µé¢„è®­ç»ƒæ–‡æœ¬..."}`

**SFT æ•°æ®**ï¼š
- æ–‡ä»¶ï¼š`dataset/sft_data.jsonl` (ç”¨æˆ·éœ€è‡ªè¡Œå‡†å¤‡)
- æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å« `conversations` å­—æ®µ
- ç¤ºä¾‹ï¼š
  ```json
  {
    "conversations": [
      {"role": "user", "content": "ä½ å¥½"},
      {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
    ]
  }
  ```

## Dependencies

**æ ¸å¿ƒä¾èµ–**ï¼š
- PyTorch >= 2.3.0 (CUDA 12.4 æ”¯æŒ)
- Transformers >= 4.44.0
- Accelerate >= 1.0.1
- PEFT >= 0.7.1
- TRL >= 0.13.0
- WandB >= 0.18.3

**å®Œæ•´ä¾èµ–**ï¼šå‚è€ƒ `pyproject.toml`ï¼ŒåŒ…å«æ•°æ®å¤„ç†ï¼ˆpandas, jieba, nltkï¼‰ã€å¯è§†åŒ–ï¼ˆmatplotlib, streamlitï¼‰ã€APIï¼ˆflask, openaiï¼‰ç­‰

## Development Notes

**å·²çŸ¥é—®é¢˜**ï¼š
- è®­ç»ƒè„šæœ¬æ‰€åœ¨ç›®å½•æ‹¼å†™ä¸º `triainer`ï¼ˆåº”ä¸º `trainer`ï¼‰
- SFT æ¨¡å‹ä¿å­˜æ–‡ä»¶åæ‹¼å†™ä¸º `full_stf`ï¼ˆåº”ä¸º `full_sft`ï¼‰
- ä»£ç ä¸­å­˜åœ¨ä¸€äº›æ‹¼å†™é”™è¯¯ï¼ˆå¦‚ `fileterwarning`, `learing_rate`ï¼‰

**é‡è¦ä¿®å¤ (2025-10-18)**ï¼š
- âœ… **Loss æ˜¾ç¤ºé€»è¾‘é”™è¯¯å·²ä¿®å¤**ï¼šæ—§ç‰ˆæœ¬ä¸­è®­ç»ƒæ—¥å¿—æ˜¾ç¤ºçš„ loss è¢«é”™è¯¯åœ°ä¹˜ä»¥äº† `accumulation_steps`ï¼Œå¯¼è‡´æ˜¾ç¤ºå€¼å¼‚å¸¸é«˜ï¼ˆ200+ï¼‰ã€‚å®é™…è®­ç»ƒæ˜¯æˆåŠŸçš„ï¼ŒçœŸå® loss åœ¨æ­£å¸¸èŒƒå›´ï¼ˆ7-9ï¼‰ã€‚
- âœ… **éªŒè¯ç»“æœ**ï¼šé¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•æ˜¾ç¤º loss ä» 8.92 é™è‡³ 7.42ï¼ˆ16.81% æ”¹å–„ï¼‰ï¼Œå›°æƒ‘åº¦ä» 7445 é™è‡³ 1664ã€‚
- âœ… **æ¨¡å‹ forward() æ–¹æ³•**ï¼šæ·»åŠ äº†é»˜è®¤å‚æ•°ï¼Œä½¿å…¶å…¼å®¹ transformers çš„ generate() æ–¹æ³•ã€‚
- ğŸ“„ **è¯¦ç»†åˆ†æ**ï¼šå‚è§ `LOSS_ANALYSIS.md`

**ç¯å¢ƒå…¼å®¹æ€§**ï¼š
- æ”¯æŒ CPU å’Œ GPU è®­ç»ƒï¼Œè‡ªåŠ¨æ£€æµ‹ CUDA å¯ç”¨æ€§
- è‡ªåŠ¨å¤„ç† Jupyter/Colab ç¯å¢ƒçš„ç‰¹æ®Šå‚æ•°
- å…¼å®¹ Kaggle ç¯å¢ƒçš„ `__file__` æœªå®šä¹‰é—®é¢˜

**ä¼˜åŒ–å»ºè®®**ï¼š
- é¢„è®­ç»ƒæ—¶ä½¿ç”¨è¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°ï¼ˆ32ï¼‰å’Œå­¦ä¹ ç‡ï¼ˆ5e-4ï¼‰
- SFT æ—¶ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆ5e-7ï¼‰é¿å…ç¾éš¾æ€§é—å¿˜
- MoE æ¨¡å¼ä¼šå¢åŠ çº¦ 4x å‚æ•°é‡å’Œè®¡ç®—é‡
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å¯ä»¥æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°
- æœ¬é¡¹ç›®æ˜¯è¿è¡Œåœ¨äº‘ç«¯çš„ æ‰€ä»¥ä¸ä¸»å¼ åœ¨æœ¬åœ°è·‘å®Œæ•´æµ‹è¯•, å¦‚æœæˆ‘ä»¬éœ€è¦åšå®é™…ä¸Šçš„æ”¹åŠ¨ è¯·ä½ åœ¨æ”¹åŠ¨å®Œå°±æ˜¯ä¸Šä¼ git å¹¶push